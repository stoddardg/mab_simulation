{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EGreedyMAB:\n",
    "    \n",
    "    def __init__(self, epsilon=.1):\n",
    "        self.arm_feedback = {}\n",
    "        self.arm_plays = {}\n",
    "        self.arm_mean_payoff = {}\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        \n",
    "    def get_decision(self,arm_id_list, arm_feature_list):\n",
    "        np.random.shuffle(arm_id_list)\n",
    "        current_averages = {id: self.arm_mean_payoff.get(id,100) for id in arm_id_list}\n",
    "        \n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(arm_id_list)\n",
    "        else:\n",
    "            return max(current_averages, key=current_averages.get)\n",
    "    \n",
    "    def update(self, arm_id, reward):\n",
    "        self.arm_feedback[arm_id] = self.arm_feedback.get(arm_id,0) + reward\n",
    "        self.arm_plays[arm_id] = self.arm_plays.get(arm_id,0) + 1.0\n",
    "        self.arm_mean_payoff[arm_id] = self.arm_feedback[arm_id] / self.arm_plays[arm_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import *\n",
    "from scipy.stats import beta\n",
    "\n",
    "class BetaBandit(object):\n",
    "    def __init__(self, num_options=2, prior = (.5, .5)):\n",
    "        self.arm_plays = {}\n",
    "        self.successes = {}\n",
    "        self.num_options = num_options\n",
    "        self.prior = prior\n",
    "    \n",
    "    def update(self, arm_id, success):\n",
    "        self.arm_plays[arm_id] = self.arm_plays.get(arm_id, 0) + 1\n",
    "        if (success==1):\n",
    "            self.successes[arm_id] = self.successes.get(arm_id, 0) + 1\n",
    "     \n",
    "    def get_decision(self,arm_id_list,arm_feature_list):\n",
    "        sampled_theta = []\n",
    "        for i in arm_id_list:\n",
    "            dist = beta(self.prior[0]+self.successes.get(i, 0),\n",
    "                       self.prior[1]+self.arm_plays.get(i,0)-self.successes.get(i,0))\n",
    "            sampled_theta += [dist.rvs()]\n",
    "        return sampled_theta.index(max(sampled_theta))   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to change weight assigned to succeses/failures, change the prior\n",
    "can weight the prior differently\n",
    "k*alpha, k*beta\n",
    "k can be greater than 1\n",
    "\n",
    "change the simulator to generate arm successes with a Beta Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "# Scales the arm_probs to all be between 0 and 1.\n",
    "# The highest pre-scaled value is assigned a value of 1.\n",
    "# Probs is a dictionary\n",
    "def scale(probs, lower_bound, upper_bound):\n",
    "    # min_int\n",
    "    biggest = - sys.maxint - 1\n",
    "    interval = upper_bound - lower_bound\n",
    "    \n",
    "    for i in range(len(probs)): \n",
    "        if abs(probs[i]) > biggest:\n",
    "            biggest = abs(probs[i])\n",
    "    biggest = biggest * 2 / interval\n",
    "    for i in range(len(probs)):\n",
    "        probs[i] = probs[i]/biggest + (.5*(upper_bound + lower_bound))\n",
    "    \n",
    "    return probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UCB():\n",
    "    def __init__(self):\n",
    "        self.arm_plays = {}\n",
    "        self.arm_rewards = {}\n",
    "        self.total_plays = 1\n",
    "    \n",
    "    def get_decision(self,arm_id_list,arm_feature_list):\n",
    "        ucb_values = {}\n",
    "        for arm in arm_id_list:\n",
    "            bonus = math.sqrt((2*math.log(self.total_plays))/float(self.arm_plays.get(arm, 1)))\n",
    "            ucb_values[arm] = (self.arm_rewards.get(arm, 0)/self.arm_plays.get(arm, 1)) + bonus\n",
    "        return max(ucb_values, key=ucb_values.get)\n",
    "    \n",
    "    def update(self, arm_id, reward):\n",
    "        self.total_plays += 1\n",
    "        \n",
    "        self.arm_plays[arm_id] = 1 + self.arm_plays.get(arm_id,0)\n",
    "        self.arm_rewards[arm_id] = reward + self.arm_rewards.get(arm_id, 0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleSimulator:\n",
    "    \n",
    "    def __init__(self,n_arms,loc_value, scale_value):\n",
    "        self.arm_probs = {}\n",
    "        temp_val_list = []\n",
    "        scaled_vals = []\n",
    "        for i in range(0, n_arms):\n",
    "            temp_val = np.random.normal(loc=loc_value, scale = scale_value)\n",
    "            temp_val_list.append(temp_val)\n",
    "        scaled_vals = scale(temp_val_list, lower_bound=0, upper_bound=.2)\n",
    "        for i in range(0,n_arms):\n",
    "            self.arm_probs[i] = scaled_vals[i]\n",
    "    \n",
    "    def get_available_arms(self):\n",
    "        return self.arm_probs.keys(), []\n",
    "    \n",
    "    def get_reward(self, arm_id):\n",
    "        return scipy.stats.bernoulli.rvs(self.arm_probs.get(arm_id,0))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Currently using a normal distribution\n",
    "-Change to a beta distribution (currently: np.random.normal(loc=...))\n",
    "-arm_probs[i] will = temp_val\n",
    "-Change the parameters of the Beta distribution in the Simulator class --> in the normal distribution, we had loc_value = mean, scale_value = width of variance\n",
    "-here, we need to use alpha and beta to parametrize the beta distribution\n",
    "-alpha and beta of the simulator will not equal those of the beta bandit \n",
    "-is Beta Bandit, when programmed with the SAME alpha and beta as the simulator, better than EGreedy? ...is it just worse?\n",
    "\n",
    "\n",
    "-change the loop to simulate k*simulations -- why? if you run it only once, then the regret/total reward itself is a random variable --> then, average the regret/total reward (final graph) k times, to see what a more accurate regret/total reward is\n",
    "----> with only one simulation, (.5, .5) might be better than (.5, .5)...\n",
    "-make priors >1; see what happens when beta > alpha for the prior\n",
    "\n",
    "Research Questions:\n",
    "-How strong of a prior should you use?\n",
    "-Should we be more pessimistic, or more optimistic? Should we make alpha > beta, to be optimistic? Or should we make beta > alpha, which is more pessimistic?\n",
    "--->depends on the alpha/beta of your simulator\n",
    "\n",
    "\n",
    "How to generate alpha and beta for the beta bandit?\n",
    "-prior can be weak or strong (ie big lambda vs small lambda)\n",
    "-prior can also be optimistic, pessimistic, or neutral (ie alpha > beta, beta< alpha, alpha = beta)\n",
    "-need to write a function to come up with different combinations\n",
    "-to loop through different combinations of lambda and optimism, lambda and pessimsm, etc...\n",
    "---> perhaps a for loop or funciton?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'scale' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-aee5fb50285f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleSimulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0ms2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleSimulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-99b6ca7fea39>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n_arms, loc_value, scale_value)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtemp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtemp_val_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mscaled_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_val_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupper_bound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_arms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marm_probs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'scale' is not defined"
     ]
    }
   ],
   "source": [
    "s1 = SimpleSimulator(10,4,2)\n",
    "s2 = SimpleSimulator(5,3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s1.arm_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s2.arm_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TIME_STEPS = 10000\n",
    "N_ARMS = 10\n",
    "# Arms, Mean, Standard Deviation\n",
    "sim = SimpleSimulator(N_ARMS, .5, .1)\n",
    "\n",
    "mab = EGreedyMAB()\n",
    "reward_list = []\n",
    "played = [[0 for i in range(N_ARMS)] for i in range(3)]\n",
    "\n",
    "sim = SimpleSimulator(N_ARMS, .5, .1)\n",
    "\n",
    "mab_3 = UCB()\n",
    "reward_list_3 = []\n",
    "\n",
    "mab_2 = BetaBandit()\n",
    "reward_list_2 = []\n",
    "\n",
    "mab_1 = EGreedyMAB()\n",
    "reward_list_1 = []\n",
    "\n",
    "reward_list_of_lists = [reward_list_1, reward_list_2, reward_list_3]\n",
    "mab_list_of_mabs = [mab_1, mab_2, mab_3]\n",
    "\n",
    "for t in tqdm(np.arange(TIME_STEPS)):\n",
    "    arms, arm_features = sim.get_available_arms()\n",
    "\n",
    "    for i in range(len(mab_list_of_mabs)):\n",
    "        mab = mab_list_of_mabs[i]\n",
    "        reward_list = reward_list_of_lists[i]\n",
    "        arm_to_play = mab.get_decision(arms, arm_features)\n",
    "\n",
    "        played[i][arm_to_play] += 1\n",
    "        \n",
    "        reward = sim.get_reward(arm_to_play)\n",
    "        mab.update(arm_to_play, reward)\n",
    "        reward_list.append(reward)\n",
    "    \n",
    "    #for reward_list in reward_list_of_lists:\n",
    "       # reward_list.append(reward)\n",
    "        \n",
    "for reward in reward_list_of_lists:\n",
    "    print 'total_reward', np.sum(reward)\n",
    "    print 'average_reward', np.mean(reward)\n",
    "\n",
    "\n",
    "max_payoff = max(sim.arm_probs.values())\n",
    "print 'best possible', max_payoff \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_payoff = max(sim.arm_probs.values())\n",
    "\n",
    "plt.plot(np.arange(len(reward_list_1)), np.cumsum(reward_list_1)/np.arange(1,TIME_STEPS+1.0), label = 'EGreedy')\n",
    "\n",
    "plt.plot(np.arange(len(reward_list_2)), np.cumsum(reward_list_2)/np.arange(1,TIME_STEPS+1.0), label='Beta Bandit')\n",
    "\n",
    "plt.plot(np.arange(len(reward_list_3)), np.cumsum(reward_list_3)/np.arange(1,TIME_STEPS+1.0), label = 'UCB')\n",
    "\n",
    "plt.plot(np.arange(TIME_STEPS), np.cumsum([max_payoff]*TIME_STEPS)/ np.arange(1,TIME_STEPS+1.0), label='OPT', ls='--')\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.xlabel(\"Time Step\", fontsize=14)\n",
    "\n",
    "plt.ylabel(\"Cumulative Reward\", fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plt.hist(sim.arm_probs.values(), 200,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = range(N_ARMS)\n",
    "y = played[0]\n",
    "plt.plot(x,y)\n",
    "for p in played:\n",
    "    print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "turn graph of \"# of times arm played\" into a histogram --> \n",
    "\n",
    "re-order arms so that the LEAST played arm = the 0th arm, and the MOST played arm = the nth arm - so graph is increasing\n",
    "\n",
    "make sure you have 3 different plots - one for each arm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
